<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Kunal Punera</title>
<link href="stylesheet.css" type="text/css" rel="stylesheet" />
</head>
<body>
<div>
<h2 id="leanpub-auto-kunal-punera">Kunal Punera</h2>

<h5 id="leanpub-auto-founder-at-bento-labs">Founder at Bento Labs</h5>

<p>
  <em>Data Mining, Data Products and Entrepreneurship</em>
</p>

<p>*Kunal Punera started hacking on computers at an early age in India. Inspired by
the way Google transformed internet search through indexing and information retrieval, he came to the United States to do his PhD in data mining and machine learning at UT Austin. *</p>

<p>
  <em>After for years at Yahoo Research working on diverse data problems, he joined Customer Relationship Management (CRM) startup RelateIQ, as their fourth engineer and first data scientist. At RelateIQ, Kunal built the data mining system from scratch – as well as many of the data products deployed.</em>
</p>

<p>*Recently, Kunal recently left RelateIQ to start his own company, Bento Labs. RelateIQ was acquired by Salesforce for $380M. In this interview, Kunal shares his experiences bridging from research to data science, thoughtful lessons about data science engineering, and the importance of tool making. *</p>

<h5 id="leanpub-auto-lets-start-with-where-you-are-and-where-you-came-from-starting-from-undergrad-what-was-your-journey-why-did-you-go-into-data">Let’s start with where you are and where you came from. Starting from undergrad, what was your journey? Why did you go into data?</h5>

<p>I did my undergraduate work in computer science in India. During that period I was more of a hacker, building things without being too worried about the theoretical side. At some point, towards the end of my undergraduate studies, I received many offers to work for software companies and just hack on things. I didn’t quite know much about the Master or Ph.D. program in other parts of the world. Then, one of my close friends was accepted into a Ph.D. program in the U.S., and I started hearing the vocabulary surrounding graduate schools in the U.S., GRE scores, the application process, etc. That’s when I started considering the possibility of studying further.</p>

<p>I was not really sure at first if I wanted to pursue further studies. So after finishing my undergraduate studies, I took a year off from the software companies to work with a professor and helping with his research; if I was going to commit many years of my life to research, I wanted to first see if I would enjoy the work. And what I discovered was that I loved it. I loved research just as much as I loved ad hoc hacking. The professor, Dr. Soumen Chakrabarti, and I wrote a couple of papers on data mining that ended up being published at the 2002 World Wide Web conference.  </p>

<p>The way I got started on data mining was coincidental. From my hacking experience, I was interested in databases and operating systems, but Soumen told me that OS and DB research was already pretty mature, and that there was a new field of research that involved combining artificial intelligence and data that he needed help with, so that is how I started working on data mining problems. Some of the first projects I worked on were on web mining and machine learning for the Web. I enjoyed thinking about those problems and really got into them. There was a sense that I could have a tangible effect on the lives of users through my research, which was pretty exciting.</p>

<p>To provide some context, this was back in 2001 and I had just discovered Google. It provided a real life example of “what data mining can accomplish.” The other search engine at the time, AltaVista, was not nearly as strong as Google. You could clearly see the difference in search quality. This was one of the first times that I saw how data mining could make a huge difference in the way people live their day-to-day lives, how they accessed information, and how they behaved online.</p>

<p>After working with Soumen for a year, I applied and was accepted into graduate school at the University of Texas (UT Austin). My Ph.D. advisor there, Dr. Joydeep Ghosh, gave me a lot of space to explore various problems in data mining and machine learning. It took me a little while to get into the academic mindset – I spent my first 2-3 years exploring the new country, with road trips across the U.S., and weeks spent in national parks. I also spent a lot of time at internships at industrial labs – IBM Almaden and Yahoo! Research. I finally got serious about research in my third year as a graduate student, and did some good work in my fourth and fifth years to wrap up my Ph.D.</p>

<h5 id="leanpub-auto-what-was-your-phd-in">What was your Ph.D. in?</h5>

<p>Topics in machine learning and data mining. The topic specifically was classification in the presence of structure in data. If you have structure in the data, could you use that to make learning algorithms better by enforcing constraints? I was very motivated by real world problems. My last two years of Ph.D. were funded by Yahoo! so the problems I tackled tended to be rooted in challenges Yahoo! was facing at the time: searching and indexing, classifying web pages, and trying to model user preferences and behavior. I solved a bunch of real world problems, and the thing I found in common between the different solutions was that they always exploited the structure in the data – websites are hierarchical structures, web pages as well, and people’s browsing could be modeled as Directed Acyclic Graphs (DAGs).</p>

<p>In reflection, my Ph.D. thesis topic came about more from figuring out a common thread in the different problems I worked on, as opposed to having a particular research agenda and pursuing it. That’s been my approach to research since my Ph.D. as well. I don’t have a specific agenda or an area that I’d like to advance. I don’t really feel like pushing any one particular technology.  All I want to do is solve hard and interesting problems. After my PhD., I took a full-time position at Yahoo! Research which, in those days, was almost an academic organization. It was basically a university department without the teaching load. It was perfect because I basically went from being a graduate student to a similar place, but was paid significantly more. Plus, they had the benefit of having an immense amount of data as well as great infrastructure to work with.</p>

<h5 id="leanpub-auto-what-kind-of-problems-were-you-solving-there-were-they-motivated-by-yahoo-user-facing-business">What kind of problems were you solving there? Were they motivated by Yahoo! user facing business?</h5>

<p>At Yahoo! Research, we had a pretty open charter to help direct our work. 50% of our time was to be spent making an impact for Yahoo! and the remaining half could be spent working on research problems that may not have much to do with the immediate needs of the company. That was an amazing experience because I got to touch pretty much any problem I wanted to. Since I could code as well as do research, I got to work on a lot of different data mining problems – including designing better CAPTCHAs, email spam detection, phishing detection, search engine ranking, targeting for the advertising systems, and new approaches to user modeling.</p>

<p>I spent four years at Yahoo! Research and worked on a wide variety of projects ranging from short-term ones (3-6 months) to some engagements that lasted years.</p>

<h5 id="leanpub-auto-did-you-focus-mostly-on-the-research-aspect-of-coding-or-did-you-also-deploy-your-research-such-as-spam-filters-in-production">Did you focus mostly on the research aspect of coding, or did you also deploy your research, such as spam filters, in production?</h5>

<p>Research scientists at Yahoo! were judged based not only on the amount of internal impact that we had, but also on the number of research papers we wrote, the number of external talks we gave, etc. Typically, Yahoo! Research did not own the projects I was working on, and so I had to work closely with the products teams responsible for them. Given the nature of the engagement, naturally, the product teams were hesitant to have researchers make direct changes to the code base; the products teams had a focused agenda while I had a broader agenda and different responsibilities. But I had a strong development background and wanted to build/optimize the systems end-to-end, and therefore, I felt a little frustrated.</p>

<p>Also, after years in research, I realized that academic careers require one to be an expert in a specific, narrow area. There’s a lot of pressure to become an expert at one thing, so everyone would know Kunal Punera is an expert at so-and-so. My research agenda has always been *“show me an interesting problem and I will work towards solving it.” *Over my four years at Yahoo! I moved across a lot of different types of problems and domains, ranking problems in search and ads to adversarial data mining in mail spam and CAPTCHAs. That doesn’t mesh very well with how academia expects publications and careers to progress. Eventually, I realized that a purely academic career wouldn’t sustain my interest enough. Moreover, outside Yahoo! I was watching the emergence of these interesting companies that are using data to solve important problems for people, and I really wanted to get involved with that.</p>

<p>At some point, I started considering leaving Yahoo!, and maybe starting my own company. During this process I realized that I had been away from software development for too long. During the five years of my Ph.D. work and four years at Yahoo! Research, I had written a lot of code, but code written for research doesn’t have to be production-quality; it doesn’t have to be maintainable, it isn’t typically changed by anyone else. Also, the whole world of web development had undergone considerable change since the time I had been writing systems in cgi-perl. You probably don’t even know what that is – it was the precursor to the modem web application frameworks. I realized that I had to update my knowledge about software development, especially when it came to using the open source stack.</p>

<p>So I realized that I had much to learn before I could start my own company, and that I wouldn’t be able to do it while at Yahoo! I had to go work at a place that would appreciate the fact that I had a very strong research background, but would give me the opportunity to learn stuff beyond data mining, and RelateIQ fell right in the sweet spot. The match was amazing. The founders were interested in building a company that solved key pain points around relationship management using cutting-edge data mining. Furthermore, since I was to be the fourth engineer, I would have to build everything from scratch on my own, and consequently, would learn a lot from the experience.</p>

<p>I spent two years at RelateIQ. I worked on building the data mining system from scratch – and by the time I left I had built most of the data products deployed in RelateIQ. And in the process I learnt a hell of a lot.  </p>

<h5 id="leanpub-auto-wow-how-did-you-learn-all-of-this-on-the-job-and-on-the-fly">Wow! How did you learn all of this on the job, and on the fly?</h5>

<p>My data mining background was very deep and broad, so I didn’t really have to learn any of the learning algorithms or approaches on the fly. I picked up Natural Language Processing (NLP) as needed, but once you have a decent statistical modeling background, the rest of machine learning is just variations of the same thing. Those were not an issue. But, software development skills were something I had to learn a lot about. For example, while I was a good coder, it is a different experience to work with engineers who had worked in production environments their entire careers. So, while Maven (for dependency management) was obvious to them, it was new to me. Using Guice for dependency injection was normal to them, while it was something I was picking up for the first time.</p>

<p>During my time at RelateIQ, in terms of software engineering, I learnt a lot. Sometimes I feel I didn’t learn nearly enough. I think there’s a lot more I could have worked on, but whatever I learnt, I learnt well. Whatever I know about machine learning algorithms, I learnt at Yahoo! Research. Whatever I know about software engineering, I learnt at RelateIQ.</p>

<p>I had come to RelateIQ as a stepping stone to starting something on my own. But as two years passed, RelateIQ blew up, in the sense that it was doing extremely well. So, there was a strong temptation to stay because the stock options were going to be worth a lot at some point. But then I also had some confidence that I could make something valuable of my own in the next few years. I loved the people at RelateIQ, but with a heavy heart, I made a decision to leave. If I hadn’t left now I might never have had a chance to do my own startup.</p>

<p>I left RelateIQ to do my own startup. In the last two months, I’ve just been rebuilding many of the things that had always existed at RelateIQ. I’ve been building a backend and figuring out how to get continuous deployment working, learning how to get the database to perform well – all these things which I didn’t have to do before. It’s been interesting.</p>

<h5 id="leanpub-auto-thats-amazing-it-seems-like-youve-systematically-identified-areas-of-knowledge-you-wanted-and-found-either-employment-or-career-opportunities-where-you-could-go-and-be-paid-to-learn-those-things-once-you-mastered-those-things-you-can-go-to-other-things-how-did-you-do-that">That’s amazing. It seems like you’ve systematically identified areas of knowledge you wanted and found either employment or career opportunities where you could go and be paid to learn those things. Once you mastered those things, you can go to other things. How did you do that?</h5>

<p>Silicon Valley is the place where, first of all, there’s a huge demand for the kinds of skills we have built up. I’m lucky that I’ve chosen to work on something that has a huge demand, and companies are willing to let you learn on the job as long as you can contribute back. RelateIQ, for the entire time I was there, did not have another data scientist, so I had to carry that whole load, but in return, I learnt a lot. Working in startups is always a give and take, and I think good companies in Silicon Valley understand that the employees they are trying to hire are, in general, smart, and have their own long-term goals that they want to pursue. As long as they contribute a lot back to the company – and I like to think that I did – then the companies help further the goals of the employees.</p>

<p>When I wanted to leave RelateIQ, everyone, from Adam to Steve to DJ, was very supportive. Steve wanted to introduce me to investors. DJ wanted me to come by and get his advice on the new idea. The environment was extremely supportive. We are very lucky to live in a business environment where companies don’t even think about locking employees in. There’s no notion of an employee lock-in. There’s no notion of company lock-in. There’s always flexibility.  Everyone wants to make the best possible use of their time. We all understand that we’re on Earth for a short time, and we all want to go to a company or work on things which uniquely need us. Silicon Valley is unique, and amazing in that way. We’re lucky to be in this situation.</p>

<h5 id="leanpub-auto-being-able-to-learn-new-things-really-quickly-is-one-of-the-things-we-need-today-more-than-ever-but-theres-an-art-to-doing-that-you-need-to-have-some-sort-of-foundation-core-programming-skills-core-modeling-skills-if-you-were-to-decompose-those-down-to-the-principle-skills-what-do-you-feel-is-most-important">Being able to learn new things really quickly is one of the things we need today more than ever, but there’s an art to doing that. You need to have some sort of foundation, core programming skills, core modeling skills. If you were to decompose those down to the principle skills, what do you feel is most important?</h5>

<p>In terms of programming skills, I’m not sure what the curriculum nowadays looks like, but in my undergraduate days, I started by learning C. Actually, I learnt Pascal first. Then, I learnt C. These are pretty low-level languages with few rules and close interaction with the machine. So, I learnt from the very beginning how programming languages manage memory, what pointers are, what an execution stack looks like, etc. I think that experience was useful because now, if I have to learn new concepts, it’s easy for me to go back and reconstruct them from the first principles in my head.  </p>

<p>In terms of programming, I would think learning about core programming concepts is important. You should start learning from there. I have a feeling that if you started learning programming with Javascript, it might be a little bit more difficult to know exactly what’s happening in the background. I would encourage one to learn what is happening at a low level, but also to not spend too much time on it. I spent way too many years working with C and C++. Nowadays, I wouldn’t build systems in those languages. Java, Scala, Ruby, Python have amazing framework support, open source libraries, and lots of solutions documented in sources like Stack Overflow.</p>

<p>In terms of data modeling, I think I was lucky that I took some good statistics courses. It’s useful to understand the underlying concepts of algorithms. I think a graduate-level optimization course is important, as well.</p>

<p>One of the obstacles I sometimes see engineers running into is confusing the core problem that needs to be solved and the one particular solution to that problem. Sometimes people have one way of solving the problem already in their head, and they might not see that the core problem is not the same thing as their solution. As much as possible, I would encourage people to constantly ask the question “What am I optimizing?” For example, if you want to obtain a clustering of data, it’s useful to first try to determine what properties you would want in a good solution, and then attempt to encode these criteria into a loss function. If one is not careful it is easy to think of clustering data in terms of steps the algorithm should take, or a series of methods that must be implemented. This can sometimes lead the engineer astray in that the preconceived solution might never end up obtaining clusters with the desired properties. Of course, the time constraints in a startup don’t leave data scientists the luxury of carefully thinking of every problem. In these situations, experience helps.</p>

<h5 id="leanpub-auto-do-you-have-any-specific-examples-for-that-i-know-what-youre-saying-in-the-abstract-but-it-would-be-helpful-to-hear-a-concrete-example">Do you have any specific examples for that? I know what you’re saying in the abstract but it would be helpful to hear a concrete example.</h5>

<p>For example, suppose you want to do classification. Say I have two classes, and I wanted to learn a model that separates them. I can use one of the many algorithms out there: decision trees, support vector machines (SVM), random forests, etc. But one might come to erroneously think that the classification problem is <em>equal</em> to learning decision trees – without completely understanding what underlying problem is being solved.</p>

<p>Before jumping into implementing a solution, one might want to consider some questions about  the nature of the problem. The core problem here is that there is a boundary, a separation between the two classes, that we need to find. Well, what does it mean to find a boundary? What kind of boundaries do decision trees find? And what kind of boundaries do linear SVMs find? Will using a kernel method help? Is this a situation where I need to worry about irrelevant features? Does that mean I need to regularize via a L1 norm or stick with L2? These are fundamental questions that once answered can guide us to the appropriate approach and thus avoid a lot of  trial and error. Moreover, they help in the following situation: once we have applied the first algorithm and it obtains 65% classification accuracy, what should we do next to improve the results? Carefully defining the parameters of the problems and the characteristics of a good solution help us figure out what the next step to take is.</p>

<p>Sometimes when reading Hacker News, I get a sense that people feel machine learning is simply about taking open-source libraries and applying them to data. In many cases, this works okay as the first step, but, often, the next step to further improve the model is difficult to figure out. But if one has a strong understanding of what these libraries are trying to optimize for, what each core algorithm is good for, how the curse of dimensionality effects learnability, what the difference is between L1 norm and L2 norm, and other such kinds of things, then it becomes much easier to figure out how best to apply these open source resources.</p>

<h5 id="leanpub-auto-so-is-this-the-set-of-skills-you-are-looking-for-when-you-are-interviewing-for-the-data-scientist-position">So is this the set of skills you are looking for when you are interviewing for the data scientist position?</h5>

<p>When I am looking for data scientists, the most important thing I am looking for is whether their approach to machine learning is systematic. Sometimes I meet people who know what first step to take, and can do it pretty fast because they’re amazing coders, but the second step becomes a little harder. When I interview people, I don’t really want them to solve anything on the whiteboard, I don’t want them to code. The key thing I want to know is whether they get the underlying principles of what they are building. I’ll typically ask them about something they’ve previously built and then just delve deeper and deeper into that same problem. I find this is a good way to evaluate candidates because if they contributed significantly to the work and know their fundamentals, they would be able to defend their decisions from first principles and not just say, <em>“Everyone likes SVMs so I used it.”</em> They should say,* “Well, the problem had the following properties, that’s why we needed a SVM”. <em>Or “</em>I also tried this other thing. It didn’t work well because I believe… ”* as opposed to <em>“I just didn’t try it.”</em></p>

<p>I think that’s a key thing to look for – strong fundamentals. If someone has strong fundamentals, but doesn’t know what a random forest is, I don’t really care, because individual machine learning approaches can be easily learnt.  Having a strong background and then picking up random forests is way easier than having just shallow knowledge of random forests and then trying to debug them. People who are looking to work with data mining algorithms should take a systematic approach to learning them. I think it’s difficult to do that nowadays because there’s so much demand for the skill set. But I would urge them to get more fundamental skills via, maybe, a machine learning course which focuses less on the specific algorithms and more on the fundamentals, and then some core statistics, optimization, and algorithms courses. These will give them a good foundation for their work.</p>

<h5 id="leanpub-auto-when-you-were-trying-to-build-the-data-science-team-at-relateiq-how-big-was-the-team">When you were trying to build the data science team at RelateIQ, how big was the team?</h5>

<p>At RelateIQ we were not building a <em>data science</em> team. One of the things I’m not fond of is the kind of process I used to follow at Yahoo! Research, where the science team builds the models and then passes them off to the engineers to implement or deploy. I feel like a lot gets lost in the translation. Sometimes the models that one builds assume an environment that doesn’t really exist in production, and one doesn’t know that until the models are deployed. And in fact, when models get deployed and accuracy lags, it’s very difficult for engineers who didn’t build the models to convey back what’s exactly happening.</p>

<p>I prefer that the scientists be closely involved in the implementation of the feature pipelines and the models in production. They should know how the model is deployed, everything that happens to the data – filters, sampling – before it shows up as input into the model. If there’s a particular filter which takes out one particular type of data, the scientists should know about it. Moreover, in the first few months after a model is deployed, the scientists should be the ones maintaining it.</p>

<p>At RelateIQ, we were following this principle and building a <em>data products engineering</em> team. We didn’t call it data science at all. In the data products engineering team, we looked for people who had a very strong sense of data, who liked playing with data, but also had reasonable engineering skills so that they could actually touch production code directly. We didn’t expect them to roll out their own hadoop infrastructure, though many of our data product engineering people did. But we wanted them to be able to deploy their own models, run them, write the feature extractor pipelines, etc. Apart from the principle I mentioned earlier, a second reason for building the team this way was more pragmatic; we were a small team and we couldn’t spare engineers dedicated to taking the work done by data scientists to production.</p>

<h5 id="leanpub-auto-it-seems-like-you-not-only-focused-on-people-who-can-do-data-analysis-but-also-on-those-who-have-a-strong-engineering-background-people-who-started-out-hacking-or-coding-and-then-went-to-data-science">It seems like you not only focused on people who can do data analysis but also on those who have a strong engineering background, people who started out hacking or coding and then went to data science.</h5>

<p>I find that you can go both ways: start from the data side or software development side of things. But in a startup, having people with both skill sets is critical. One thing that you never want in a startup is to have a data scientist working alone with no established process to get work into production. I’ve seen many startups where data scientists are six months ahead of production systems; they have done six months of work that hasn’t been deployed because the engineers that touch production are busy with their own work or fires. These data scientists have done the work in R or matlab and are not able to integrate it with the production backend system. You don’t want to have that situation.</p>

<p>In a slightly bigger startup, one may want to have small teams of two or three people – one data scientist, one person with engineering system type skills, and one with product management type skills – to build and maintain data products. They work as a team to build features as opposed to a data science person building a model alone, and hoping that one day some engineer is going to step forward and bring those features to production.  </p>

<p>When RelateIQ was small we avoided this situation by having one person perform all three roles – data scientist, engineer, and product management. Now that we are larger we are building multi-skilled teams.</p>

<h5 id="leanpub-auto-this-is-a-fantastic-example-of-data-science-and-product-done-right-you-avoided-some-of-the-pitfalls-of-great-locked-up-models-that-you-cant-deploy-were-there-other-things-you-saw-at-relateiq-that-you-felt-were-really-good-lessons-in-building-data-products">This is a fantastic example of data science and product done right. You avoided some of the pitfalls of great, locked up models that you can’t deploy. Were there other things you saw at RelateIQ that you felt were really good lessons in building data products?</h5>

<p>Other than the constitution of the team, another important aspect to keep in mind is the cadence of development of data products. Engineering the systems that involve data mining is a little different in the sense that most times it is not clear how many engineering resources will be needed before the models reach production quality, or even whether the desired quality can even be reached. This may not be a big problem at large companies, but it makes scheduling and resourcing data products tasks problematic when resources are constrained, as in a startup. In a startup, one should want to break down data products work such that visible, measurable progress can be made in two-three weeks so that the engineers have intermediate wins. This also prevents engineers from going too far down the wrong path. Of course, the development cycles need to be long enough so that hard problems can be attempted and solved; very short inflexible cycles typically lead to data products that have been patched together and are not robust.</p>

<p>Another important aspect relates to scheduling; if you want a data product deployed at any point in time, you probably should give the data engineers a head start so that they definitely have their models or features built before the front-end or back-end resources become available. This is simply a consequence of the uncertainty around the pace of progress on data products.</p>

<p>For a long time after I joined RelateIQ, I was the only one working on data products. In these early days, scheduling was not that much of an issue since I was the only data, backend, and frontend resource. Moreover, I have a lot of experience with data mining and was able to avoid going down bad paths, and was able to get most models deployed in the first couple of iterations. As the team grew and I had more frontend and backend resources that could help me, we had to work harder on scheduling and we applied the principles I outlined earlier.</p>

<h5 id="leanpub-auto-any-other-data-product-hacks-from-relateiq">Any other data product hacks from RelateIQ?</h5>

<p>The other thing that we did a lot is we took shortcuts. We took shortcuts all over the place. At the beginning we were in a hurry and we didn’t even know whether the products would be received well by the users, and so we tried to get away with putting in as many hacks as possible. We made our decisions on the hacks in this particular way:</p>

<p>Anything that was fundamental, core-level, functionality, such the parsing of emails, we made sure it was extremely strong. There are many reasons why functionality gets put into this category: in the case of email parsing, first every email has to be parsed and the cost to reparse is very high (I’ll have to go back and fetch every email and reparse it), and second, a whole bunch of other features of the data system depend on accurate parsing of email. Therefore, my system for parsing is very strong. It involves nice, sound models based on CRFs and SVMs that we learnt over large quantities of training data and that are continuously trained as data changes; these models are sound.</p>

<p>Other functionalities are higher level, such as automatically making a suggestion to follow-up with a contact. There are many questions that need to be answered here that are difficult to optimize using data since the degrees of freedom are too high or training data is too noisy. When a suggestion has to be created, the system has to determine if a follow-up to an email is warranted, whether the user has already done the follow-up, how long the system should wait before reminding the user to follow-up, and if multiple users are referenced in the email that the suggestion to follow-up will be directed to. The dimensionality of this space of choices is so high that the first attempt to model this should involve using manual rules and hard thresholds.</p>

<p>Another example is trying to learn the effectiveness of our rules for follow-up suggestions via usage data, and using the feedback if a user rejects the suggestion. Even this is complicated since rejection is a very aggregate action and it’s not clear what the user is rejecting; maybe we made a mistake in parsing the email and no suggestion was warranted or maybe the user liked the suggestion but we made it too early, or the user doesn’t like the sender of the email, or even the user hates all suggestions in general. So even here I shied away from modeling the entire problem and put in a lot of rules, a lot of very simple models. The first cut solution involved counting the number of rejections and rules for actions to be taken when certain thresholds are met. As the data product improved we used more advanced approaches to model user feedback.</p>

<p>Yet another important aspect is tools, and this is something that was driven home to me at RelateIQ. Before working at RelateIQ, I had a very high threshold for tooling. Sometimes I ended up doing the same thing 10 times without automating it because every time I did it, I was not sure I would be doing it again. After RelateIQ I can safely say that if someone does something 3 or 4 times, they will be doing it again in the future, so they should try to automate it; automate data cleaning scripts, automate model deployments, write tools for re-training of models, don’t do it by hand. Write tools that will automatically create a fresh data set, retrain the model, check its accuracy, and send you an email if the accuracy is below a threshold, and, if the accuracy is good then deploy the model. This tooling might seem excessive but is going to easily pay for itself in terms of saved time in the long run.</p>

<h5 id="leanpub-auto-was-that-a-change-of-pace-compared-to-when-you-were-working-in-research">Was that a change of pace compared to when you were working in research?</h5>

<p>Well, I did a lot of work at Yahoo! Research. I worked on new projects every 3 to 6 months. I was writing three to four papers every year and that’s a lot of work. I’m used to working a lot of late hours. At RelateIQ what changed was the emphasis. At Yahoo! Research, the emphasis was always on doing something innovative. It was all about asking, *“Last year Microsoft Research published this. The year before, Google did. What’s the new angle I can find and solve the problem?” *Sometimes the problem being considered was not an immediate concern for Yahoo. Other times the problem could be solved using simpler means. At these points, we would consider more complex versions of the problem with additional hurdles, and then figure out how to solve them. The goal was to constantly push the envelope of what was possible with data mining, and not just to solve immediate practical problems. The task was as much to find new problems as to solve them.</p>

<p>At RelateIQ, I worked extremely hard as well. There, the problem to be solved was pretty clear, and main question facing me was <em>“What is the minimum effort that I can put forth and get something into the hands of the users so that I can test whether the solution is useful?”</em> And from that feedback I can figure out how much more effort I want to put into it in the future to improve the feature. Moreover, the solutions I tried out were chosen not just for their innovativeness, but using a tradeoff between effectiveness and the cost of implementation and future maintenance.</p>

<p>So the main difference from the earlier days of research was not one of pace of work. It was a change in priorities.</p>

<h5 id="leanpub-auto-how-do-you-measure-cost-development-effort-or-time">How do you measure cost? Development effort or time?</h5>

<p>Cost in this case involves implementation and future maintenance. In a startup you have to constantly trade that off for accuracy of models. If one option is a complex model such as a Conditional Random Field (CRF), but we can come up within 5% of the accuracy using a Naïve Bayes model, then we would choose to go with the Naïve Bayes. It is not just that CRF models would be significantly harder to train within a typical project timeline in a startup, but as the data environment changes in the future, the CRF will break in non-intuitive ways and it won’t be easy to debug. Whereas in the Naïve Bayes model, you can look at the parameters and try to see what might be happening.</p>

<p>A big issue that impacts machine learning at startups is that manually labeled training data might be hard to come by. This is why at RelateIQ a lot of the models I had to build involved a lot of manual intervention. I had to be very careful about picking the right features that, based on my experience, would not cause me to overtrain; because I knew my training data was so biased and so limited that I could not rely on cross validation. I had to basically look at each feature and ask, “While using this feature gives me a reduction in test error, what is the chance that this is simply because of the way the data in the training set was selected?”</p>

<p>Another side effect of limited training data is that sometimes it is useful to closely examine and perhaps manually twiddle the model’s parameters, setting them to values (or signs) that intuitively make sense. This intuition has to be balanced against the parameters coming from the learning algorithms. As in most cases, extensive experience with learning algorithms in limited data situations helps.</p>

<p>Yet another impact of limited training data is the high likelihood that the training data is from a different distribution as your deployment data. For example, at the beginning the startup might have 48 customers, and they are probably all friends of the CEO. The models trained on data obtained from these customers are likely to be biased towards them. However, one year down the line, the startup might have 4,800 customers. If one is not careful, the models created in those early days will fail miserably on the new customers a year later.</p>

<h5 id="leanpub-auto-you-said-that-youre-building-out-your-own-tools-that-you-took-for-granted-at-relateiq-that-is-really-interesting-because-when-you-work-in-data-science-at-a-company-you-have-a-lot-of-things-that-are-taken-care-of-for-you-deployment-usually-is-handled-by-other-people-as-a-day-job-so-how-are-you-approaching-that-what-are-you-rebuilding-and-how-do-you-know-how-to-rebuild-it">You said that you’re building out your own tools that you took for granted at RelateIQ. That is really interesting because when you work in data science at a company, you have a lot of things that are taken care of for you. Deployment usually is handled by other people as a day job. So how are you approaching that? What are you rebuilding, and how do you know how to rebuild it?</h5>

<p>I am playing around with some ideas on mobile, app re-engagement, and advertising.  I am currently implementing the backend part. But since I learnt from RelateIQ that I need to invest in tools early, I am being careful in designing the backend “properly” from the very beginning. It plays nicely with my IntelliJ IDE; I am ensuring that I am able to run it entirely offline. When I push code to GitHub, the remote systems pick the code up, automatically compile/test it on the server, run the DB migration scripts, automatically deploy the APIs, etc. If we were to not invest in this now, then I would have to do this entire process manually, every time I deploy some small bug-fix. Moreover, I would probably make mistakes in the deployment steps (forgetting to migrate the DB) and find bugs that end up being simple deployment errors.</p>

<p>How do I learn all this? Some of this I worked on, but a lot of these technologies were just words that I heard while at RelateIQ. I was working with these extremely talented engineers, and I heard them talk about Docker or Maven or Guice all the time. So when I left and started working on my own company, I Googled all this stuff. Google, along with sites like Stack Overflow, is a great resource for these things.</p>

<p>And if all else fails, there’s GChat so I can ping my ex-colleagues who are friends of mine. These guys have so many years of experience that even without my completely describing the situation, they are able to point me in the right direction. I did the same thing when I moved to RelateIQ and I was the only data scientist. Since I couldn’t talk to anyone at RelateIQ about some of the problems I was tackling, I would ping my friends from Yahoo! Research to ensure that I was thinking about the problem and solution the right way. In turn I would help them think through data mining problems they were thinking of. I have been pretty shameless about asking people for help, because for any topic I am working on, except maybe one or two topics, there is someone out there who knows it better than me.</p>

<p>If you want to start your own company, you have to build the first version of the product. Eventually, you’ll get funding and be able to hire amazing engineers, who are going to scoff at my code and change everything, and I’m okay with that. But in the meantime, I need to build this. And there’s never been a better time to build things yourself – there are a lot of good tools out there. You can use the Google App Engine for example, and many aspects of the backend are abstracted away. They have their own version of Task Queue, their own databases. You don’t even know how your data is hosted. I didn’t want to use Google App Engine because it abstracts way too much, and I felt like the lock-in might be rather severe. That’s why I am building the backend from scratch on Digital Ocean.</p>

<p>The other reason to build all this myself is that it helps me interview people.</p>

<p>One day, I will have to interview someone who will help me with DevOps. It is significantly harder to interview someone if I don’t know much about DevOps myself. My advice to anyone who’s thinking of leaving a company and starting their own is that six months before you plan to leave, talk to your CTO or your manager. Tell them what your career goals are, and that you want to do this. Have them put you in situations where you can learn some of this stuff because there’s nothing better than learning on the job. Someone’s paying you, and you’re learning while doing work for them. Like I said earlier, Silicon Valley is good at giving you opportunities to learn and extend yourself.</p>

<h5 id="leanpub-auto-what-are-the-opportunities-you-see-in-data-science-right-now">What are the opportunities you see in data science right now?</h5>

<p>There are many different aspects of data science. One is data analysis, to support business decisions. There is of course a huge need for data analysts in all sorts of businesses; at RelateIQ we need data scientists to analyze our product usage and SaaS business and suggest ways to improve the product or sales processes. But there is also a huge opportunity to actually build a layer between these data scientists and the data. These guys would prefer to use higher-level statistical languages such as R, but they want their analysis code to run on large-scale data on a distributed set of machines. There are a bunch of companies looking to provide this interface between the scientists and data, so that they write their analysis in R and don’t have to worry about where it runs, and how it runs. The interface might even contain features that might help data scientists collaborate and make them much more productive. Mode Analytics and Sense are two of the newer companies I have seen in this market.</p>

<p>In terms of products enabled by data mining and machine learning, there are <em>huge</em> opportunities out there. Some are in the usual areas: Digital advertising, Search, and Recommendation systems. There are some mature players in these areas, providing both one-off services and platforms, but there is plenty of novel work coming out of startups as well.</p>

<p>One generic area that is seeing a lot of work is in trying to make sense of unstructured data. In the most straightforward cases, some startups are trying to automatically understand web pages and construct APIs over their data. However, at an abstract level this is what RelateIQ is doing as well. RelateIQ is trying to mine people’s communications data and give them insights about their own data. RelateIQ takes a mix of structured (phone call metadata) and unstructured information (email texts) and tries to extract structured objects that are of interest to the user (follow-up suggestions, new phone numbers for contacts, best connections to use to reach people etc).</p>

<p>While RelateIQ is mining relationship data sitting within enterprises, there is a huge opportunity to mine all sorts of unstructured data within enterprises and make it useful to them. For example, data within emails, calendars, etc. could be used to help large enterprises grow and maintain their talent; many startups are pursuing this.</p>

<p>Another area where data mining can help is by helping people deal with information overload. Right now there is all this news just flying by me. I always feel like I’m missing out on so much and so I need help consuming this. There are some companies trying to help with this, trying to use machine learning to do that. Have you seen Prismatic?</p>

<h5 id="leanpub-auto-ive-heard-of-them">I’ve heard of them.</h5>

<p>I downloaded the app a little while ago, and with some help from me it was able to deliver some relevant stories to me. However, it wasn’t quite as relevant as I would have liked and it wasn’t helping with the problem of me feeling that I’m missing out on a lot of good content, and, recently, I stopped using the app. Another potential example is Google that knows so much about me. If I was an Android user, they would know everything about my mobile use. I use Chrome, so they know about my browser use. I use Google Docs and Gmail so they have all my work data as well. I use a search tool heavily, so they have the set of things I am interested in as well. Given all this information, Google is in a position to completely personalize the web for me. Have you seen the movie <em>“Her”</em>? Other than the falling-in-love-with-a-robot part, why is the rest of that movie not a reality?</p>

<p>I think the technology to build much of that is already here. The data is siloed so maybe that’s an issue. Maybe the user appetite to engage with the app in such a personal way is not there yet. I feel people may not be ready to give up that much control, but I think it is headed there. I think the next big thing is going to be in that vein. The way RelateIQ works for salespeople, there will be digital services that help regular people live. There’s a soccer mom somewhere being driven insane by having to run her household, arrange for her kids to attend school and various activities, managing events for the entire family and interacting with her friends, all at the same time. Why is her phone not figuring all this out for her – making her life easier for her?</p>

<p>A dominant trend in the world is the move towards mobile. A lot of the mobile world right now is replicating what the desktop world did. On the desktop we have websites. And so we have a notion of apps on mobile. We used to navigate between websites, often through searches. So now we are devising a way to navigate across apps via deeplinks. However, it seems to me that my usage of my mobile device is very different from my usage of my laptop. I do most of my information access on my mobile device right now. I use my laptop for coding and for long-running information searches like buying something. It seems like technologies that make my life on the laptop easier may not necessarily work on mobile devices. I don’t know the answers here yet, but I feel like data mining has a large role to play. This interests me a lot and I am likely going to select a problem in this space for my startup: a mobile frontend with an intelligent backend.</p>

<p>So there are plenty of huge opportunities; though, I think they are very difficult to predict and quantify.</p>
</div>
</body>
</html>
